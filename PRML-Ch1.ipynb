{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1 Introduction\n",
    "## 1.1 Example: Polynomial Curve Fitting\n",
    "Now suppose that we are given a training set comprising $N$ observations of $x$, written $\\mathbf{x} = (x_1,\\cdots,x_N)^T$, together with corresponding observations of the values of t, denoted $\\mathbf{t} = (t_1,\\cdots,t_N)^T$. Our goal is to exploit this training set in order to make predictions of the value $\\hat{t}$ of the target variable for some new value $\\hat{x}$ of the input variable. For a given $\\hat{x}$, there is uncertainty as to the appropriate value for $\\hat{t}$.\n",
    "\n",
    "Loss function without regularization\n",
    "$$E(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^N \\{y(x_n,\\mathbf{w}) - t_n\\}^2$$\n",
    "\n",
    "Loss function with regularization\n",
    "$$\\tilde{E}(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^N \\{y(x_n,\\mathbf{w}) - t_n\\}^2 + \\frac{\\lambda}{2}||\\mathbf{w}||^2$$\n",
    "The particular case of a quadratic regularizer is called *ridge regression*. In the context of neural networks, this approach is known as *weight decay*.\n",
    "## 1.2 Probability Theory\n",
    "**The Rules of Probability**\n",
    "\n",
    "**sum rule** \n",
    "$$P(X) = \\sum_Y p(X,Y)$$\n",
    "**product rule**\n",
    "$$P(X,Y) = p(Y|X)p(X)$$\n",
    "\n",
    "**Bayes' theorem**\n",
    "$$p(Y|X) = \\frac{p(X|Y)}{p(X)}p(Y)$$\n",
    "where the denominator can be expressed in terms of the quantities appearing in the numerator\n",
    "$p(X) = \\sum_Y p(X|Y)p(Y)$.\n",
    "\n",
    "**Independent** \n",
    "$$P(X,Y) = P(X)P(Y)\\text{    or    } P(Y|X) = P(Y)$$ \n",
    "\n",
    "### Expectation\n",
    "$\\mathbb{E}[f] = \\int p(x)f(x)dx$ (wenzhe: population, action)\n",
    "\n",
    "$\\mathbb{E}_x[f(x,y)]$ is a function of $y$, which denotes the average of the function $f(x,y)$ with respect to the distribution of x\n",
    "\n",
    "$\\mathbb{E}_x[f|y] = \\int p(x|y)f(x) dx$ conditional expection\n",
    "\n",
    "### Variance\n",
    "$\\mathbb{var}[f] = \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])^2] = \\mathbb{E}[f(x)^2] - \\mathbb{E}[f(x)]^2$\n",
    "### Covariance\n",
    "$\\mathbb{cov}[x,y] = \\mathbb{E}_{x,y}[(x-\\mathbb{E}[x])(y - \\mathbb{E}[y])]$\n",
    "\n",
    "### Bayesian probabilities\n",
    "Bayesian view provides a quantification of uncertainty. We capture our assumptions about $\\mathbf{w}$, before observing the data, in the form of a prior probability distribution $p(\\mathbf{w})$. The effect of the observed data $\\mathcal{D} = \\{t_1,\\cdots,t_N\\}$ is expressed through the conditional probability $p(\\mathcal{D}|\\mathbf{w})$.\n",
    "$$p(\\mathbf{w}|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})}$$\n",
    "\n",
    "which allows us to evaluate the uncertainty in $\\mathbf{w}$ after we observed $\\mathcal{D}$ in the form of the posterior probability $p(\\mathbf{w}|\\mathcal{D})$. The likelihood function $p(\\mathcal{D}|\\mathbf{w})$ expresses how probable the observed data set is for different settings of the parameter vector $\\mathbf{w}$.\n",
    "\n",
    "Now suppose that we have a data set of observations $\\mathbf{x} = (x_1,\\cdots,x_N)^T$ and this set is i.i.d., the maximum likelihood solution is given by the sample mean, and sample variance (measured with respect to the sample mean)\n",
    "$$\\mu_{ML}\\frac{1}{N}\\sum_{n=1}^Nx_n$$\n",
    "$$\\sigma_{ML}^2 = \\frac{1}{N}\\sum_{n=1}^N(x_n-\\mu_{ML})^2$$\n",
    "which random variables depending on the data set. Their expectations are given below.\n",
    "$$\\mathbb{E}[\\mu_{ML}] = \\mu$$\n",
    "$$\\mathbb{E}[\\sigma_{ML}^2] = \\frac{N-1}{N}\\sigma^2$$. The unbiased variance estimator is \n",
    "$$\\tilde{\\sigma}^2 = \\frac{N}{N-1}\\sigma_{ML}^2$$\n",
    "\n",
    "We assume that, given the value of $x$, the corresponding value of $t$ has a Gaussian distribution with a mean equal to the value $y(x,\\mathbf{w})$ of the polynomial curve given by\n",
    "$$p(t|x,\\mathbf{w},\\beta) = \\mathcal{N}(t|y(x,\\mathbf{w}),\\beta^{-1})$$\n",
    "\n",
    "which is a probabilistic model, expressed in terms of the *predictive distribution*. This model gives the probability distribution over t, rather than simply a point estimate.\n",
    "$$p(t|x,\\mathbf{w}_{ML},\\beta_{ML}) = \\mathcal{N}(t|y(x,\\mathbf{w}_{ML}),\\beta_{ML}^{-1})$$ \n",
    "\n",
    "Given the Gaussian prior $p(\\mathbf{w}|\\alpha) = \\mathcal{N}(\\mathbf{w}|\\mathbf{0},\\alpha^{-1}\\mathbf{I})$, maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-squares error function with a regularization parameter given by $\\lambda = \\alpha/\\beta$.\n",
    "\n",
    "### Bayesian curve fitting\n",
    "Although we have included a prior distribution $p(\\mathbf{w}|\\alpha)$, we so far still making a point estimate of $\\mathbf{w}$ and so this does not yet amount to a Bayesian treatment. In a fully Bayesian approach, we should consistently apply the sum and product rules of probability, which requires that we integrate over all values of $\\mathbf{w}$. Such merginalizations lie at the heart of Bayesian methods for pattern recognition.\n",
    "\n",
    "The predictive distribution is given as follows ($\\mathbf{x}$ and $\\mathbf{t}$ are observed data, $x$ is a new data, $t$ is the prediction for $x$)\n",
    "$$p(t|x,\\mathbf{x},\\mathbf{t}) = \\int p(t|x,\\mathbf{w})p(\\mathbf{w}|\\mathbf{x},\\mathbf{t})d\\mathbf{w}$$\n",
    "\n",
    "where $p(t|x,\\mathbf{w}) = \\mathcal{N}(t|y(x,\\mathbf{w}),\\beta^{-1})$, $p(\\mathbf{w}|\\mathbf{w},\\mathbf{t})$ is found by normalizing $p(\\mathbf{t}|\\mathbf{x},\\mathbf{w},\\beta)p(\\mathbf{w}|\\alpha)$. (wenzhe: randomness comes from the observed data ($\\mathbf{x}$, $\\mathbf{t}$)) and the prediction distribution ($t$).\n",
    "\n",
    "We see that the variance, as well as the mean, of the predictive distribution is dependent on $x$. The first term in represents the uncertainty in the predicted value of $t$ due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution through $\\beta_{ML}^{-1}$. However, the second term arises from the uncertainty in the parameters $\\mathbf{w}$ and is a consequence of the Bayesian treatment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1140eec90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def fig1d2(N = 10,scale = 0.25):\n",
    "    tx = np.linspace(0,1,100)\n",
    "    ty = np.sin(2*np.pi*tx)\n",
    "    plt.plot(tx,ty,'g-')\n",
    "    x = np.linspace(0,1,N)\n",
    "    y = np.sin(2*np.pi*x) + np.random.normal(scale = scale, size = N)\n",
    "    plt.plot(x,y,'bo')\n",
    "    plt.ylim([-1.5,1.5])\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.show()\n",
    "fig1d2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model Selection\n",
    "## 1.4 The Curse of Dimensionality\n",
    "In this book, we shall make extensive use of illustrative examples involving input spaces of one or two dimensions, because this makes it particularly easy to illustrate the techniques graphically. The reader should be warned, however, that not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions.\n",
    "## 1.5 Decision Theory\n",
    "Here we turn to a discussion of decision theory that, when combined with probability theory, allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition. Determination of $p(\\mathbf{x},\\mathbf{t})$ from a set of training data is an example of *inference*. \n",
    "\n",
    "For **cancer diagnosis** example, the general inference problem then involves determining the joint distribution $p(\\mathbf{x}, \\mathcal{C}_k)$, which gives us the most complete probabilistic description of the situation. Although this can be a very useful and informative quantity, in the end we must decide either to give treatment to the patient or not, and we would like this choice to be optimal in some appropriate sense. This is the *decision* step.\n",
    "$$p(\\mathcal{C}_k|\\mathbf{x}) = \\frac{p(\\mathbf{x}|\\mathcal{C}_k)}{p(\\mathbf{x})}p(\\mathcal{C}_k)$$\n",
    "\n",
    "For many applications, our objective will be more complex than simply minimizing the number of misclassifications. The optimal solution is the one which minimizes the loss function.\n",
    "$$\\mathbb{E}[L] = \\sum_k\\sum_j \\int_{\\mathcal{R}_j} L_{kj}p(\\mathbf{x},\\mathcal{C}_k)d\\mathbf{x}$$\n",
    "The decision rule that minimizes the expected loss is the one that assigns each new $\\mathbf{x}$ to the class $j$ for which the quantity\n",
    "$$\\sum_k L_{kj}p(\\mathcal{C}_k|\\mathbf{x})$$\n",
    "is minimum. Hence finding $p(\\mathcal{C}_k|\\mathbf{x})$ is necessary.\n",
    "\n",
    "### Inference and decision\n",
    "We have broken the classification problem down into two separate stages, the *inference stage* in which we use training data to learn a model for $p(\\mathcal{C}_k|\\mathbf{x})$, and the subsequent *decision* stage in which we use these posterior probabilities to make optimal class assignments. An alternative possibility would be to solve both problems together and simply learn a function that maps inputs $\\mathbf{x}$ directly into decisions. Such a function is called a *discriminant function*.\n",
    "\n",
    "**generative models** explicitly or implicitly model the distribution of inputs as well as outputs ($p(\\mathbf{x},\\mathcal{C}_k)$).\n",
    "\n",
    "**discriminative models** models the posterior probabilities directly ($p(\\mathcal{C}_k | \\mathbf{x})$), followed by the decision problem.\n",
    "\n",
    "**discriminant function** maps each input $\\mathbf{x}$ directly onto a class label. With this approach, however, we no longer have access to the posterior probabilities $p(\\mathcal{C}_k|\\mathbf{x})$, which is needed by minimizing risk, reject option, conpensating for class priors, combining models.\n",
    "\n",
    "### Loss functions for regression\n",
    "For regression problem, the decision stage consists of choosing a specific estimate $y(\\mathbf{x})$ of the value of $t$ for each input $x$. The expected loss is give by\n",
    "$$\\mathbb{E}[L] = \\iint L(t,y(\\mathbf{x}))p(\\mathbf{x},t)d\\mathbf{x}dt$$\n",
    "A common choice of loss function is the squared loss given by $L(t,y(\\mathbf{x})) = \\{y(\\mathbf{x})-t\\}^2$. The estimate that minimizes the expected loss is $\\mathbb{E}_t[t|\\mathbf{x}]$. Inserting back to the expected loss gives\n",
    "$$\\mathbb{E}[L] = \\int\\{y(\\mathbf{x})-\\mathbb{E}[t|\\mathbf{x}]\\}^2p(\\mathbf{x})d\\mathbf{x} + \\int\\{\\mathbb{E}[t|\\mathbf{x}]-t\\}^2p(\\mathbf{x})d\\mathbf{x}$$\n",
    "where the second term represents the intrinsic variability of the target data and can be regarded as noise (irreducible loss).\n",
    "## 1.6 Information Theory\n",
    "The average amount of information that they transmit in the process (entropy) is given by\n",
    "$$H[x] = -\\sum_x p(x)\\log_2p(x)$$\n",
    "which is also a lower bound on the number of bits needed to transmit the state of a random variable. For continuous RV, the entropy differential is given by\n",
    "$$H[x] = -\\int p(\\mathbf{x})\\ln p(\\mathbf{x})d\\mathbf{x}$$\n",
    "\n",
    "The distribution that maximizes the differential entropy is the Gaussian. The differential entropy, unlike the discrete entropy, can be negative.\n",
    "\n",
    "The additional information needed to specify the corresponding value of $\\mathbf{y}$, given the value of $\\mathbf{x}$ is given by\n",
    "$$H[\\mathbf{y}|\\mathbf{x}] = -\\iint p(\\mathbf{y},\\mathbf{x})\\ln p(\\mathbf{y}|\\mathbf{x})d\\mathbf{y}d\\mathbf{x}$$\n",
    "$$H[\\mathbf{x},\\mathbf{y}] = H[\\mathbf{y}|\\mathbf{x}] + H[\\mathbf{x}]$$\n",
    "\n",
    "Consider some unknown distribution $p(\\mathbf{x})$, and suppose that we have modelled this using an approximating distribution $q(\\mathbf{x})$. If we use $q(\\mathbf{x})$ to construct a coding scheme for the purpose of transmitting values of $\\mathbf{x}$ to a receiver, then the average *additional* amount of information required to specify the value of $\\mathbf{x}$ as a result of using $q(\\mathbf{x})$ instead of the true distribution $p(\\mathbf{x})$ is given by (Kullback-Leibler divergence / relative entropy)\n",
    "$$KL(p||q) = -\\int p(\\mathbf{x})\\ln\\frac{q(\\mathbf{x})}{p(\\mathbf{x})}$$\n",
    "Properties $KL(p||q) \\neq KL(q||p) \\geq 0$\n",
    "KL divergence is a measure of the dissimilarity of the two distribution $p(\\mathbf{x})$ and $q(\\mathbf{x})$.\n",
    "\n",
    "The mutual information is given by\n",
    "$$I[\\mathbf{x},\\mathbf{y}] = \\text{KL}(p(\\mathbf{x},\\mathbf{y})||p(\\mathbf{x})p(\\mathbf{y})) = H[\\mathbf{x}] - H[\\mathbf{x}|\\mathbf{y}] = H[\\mathbf{y}] - H[\\mathbf{y}|\\mathbf{x}]$$\n",
    "\n",
    "From a Bayesian perspective, the mutual information represents the reduction in uncertainty about $\\mathbf{x}$ as a consequence of the new observation $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
